AWSTemplateFormatVersion: 2010-09-09
Description: An Amazon S3 Tables Bucket Migration Solution.
Metadata:
  Version: '1.0.0'
  License:
    Description: >-
      'MIT No Attribution

      Copyright Amazon.com, Inc. or its affiliates. All Rights Reserved.

      Permission is hereby granted, free of charge, to any person obtaining a copy of
      this software and associated documentation files (the "Software"), to deal in
      the Software without restriction, including without limitation the rights to
      use, copy, modify, merge, publish, distribute, sublicense, and/or sell copies of
      the Software, and to permit persons to whom the Software is furnished to do so.

      THE SOFTWARE IS PROVIDED "AS IS", WITHOUT WARRANTY OF ANY KIND, EXPRESS OR
      IMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY, FITNESS
      FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT. IN NO EVENT SHALL THE AUTHORS OR
      COPYRIGHT HOLDERS BE LIABLE FOR ANY CLAIM, DAMAGES OR OTHER LIABILITY, WHETHER
      IN AN ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING FROM, OUT OF OR IN
      CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER DEALINGS IN THE SOFTWARE.'


  AWS::CloudFormation::Interface:

    ParameterGroups:

      -
        Label:
          default: "Existing Data, Database and Table details"
        Parameters:
          - YourS3Bucket
          - YourExistingGlueDatabase
          - YourExistingGlueTable      

      -
        Label:
          default: "Destination S3 Tables Bucket ARN, Namespace, Table and Table Partitions"
        Parameters:
          - S3TableBucket
          - S3TableBucketNamespace
          - S3TableBucketTables
          - S3TableBucketTablesPartitions

      -
        Label:
          default: "Please choose your desired Migration Type"
        Parameters:
          - MigrationType

         
      -
        Label:
          default: "Job Notification and Tracking"
        Parameters:
          - RecipientEmail
                       
      -
        Label:
          default: "EMR Cluster Performance"
        Parameters:
          - ClusterSize

      -
        Label:
          default: "EMR Instance Networking and Primary Node Keypair"
        Parameters:
          - subnetIDs
          - KeyPair




    ParameterLabels:
      YourS3Bucket:
        default: "The Source S3 Bucket Containing your data files"
      S3TableBucket:
        default: "Your destinations S3 Tables Bucket ARN"


Parameters:

  YourS3Bucket:
    Description: Please enter the name of the bucket containing the data you want to migrate
    Type: String
    AllowedPattern: '^[a-z0-9.-]{3,63}$'
    ConstraintDescription: Bucket name must contain only lowercase letters, numbers, periods (.), and dashes (-). Visit Amazon S3 User Guide 'https://docs.aws.amazon.com/AmazonS3/latest/userguide/bucketnamingrules.html' 

  
  S3TableBucket:
    Description: Please enter the ARN of your S3 Tables Bucket
    Type: String
    MinLength: '3'
    ConstraintDescription: Please provide your desired valid S3Tables Bucket ARN
    AllowedPattern: '(arn:aws[-a-z0-9]*:[a-z0-9]+:[-a-z0-9]*:[0-9]{12}:bucket/[a-z0-9_-]{3,63})'


  S3TableBucketNamespace:
    Description: Please specify the S3Tables Namespace or Database where your data will be stored in your Amazon S3 Tables Bucket
    Type: String
    MinLength: '0'
    MaxLength: '1024'    
    ConstraintDescription: Please provide your desired S3Tables Bucket namespace
    AllowedPattern: '[0-9a-z_]*'

   

  S3TableBucketTables:
    Description: Please enter your desired S3 Tables Table Name
    Type: String
    MinLength: '3'
    MaxLength: '63'  
    ConstraintDescription: Please provide your Existing S3Tables Bucket ARN
    AllowedPattern: '[0-9a-z_]*'


  S3TableBucketTablesPartitions:
    Description: Please enter your desired S3 Tables Table Partitions. This is needed since CTAS does not preserve source Partition information
    Type: String
    Default: NotApplicable
    ConstraintDescription: Please provide your desired destination S3Table Table Partition information or type NotApplicable to ignore


  YourExistingGlueDatabase:
    Description: Please enter the ARN of your S3 Tables Bucket
    Type: String
    MinLength: '1'
    MaxLength: '255'    
    ConstraintDescription: Please provide your Existing Glue Database name or Namespace
    AllowedPattern: '[\u0020-\uD7FF\uE000-\uFFFD\uD800\uDC00-\uDBFF\uDFFF\t]*'


  YourExistingGlueTable:
    Description: Please enter the ARN of your S3 Tables Bucket
    Type: String
    MinLength: '1'
    MaxLength: '255'     
    ConstraintDescription: Please provide your Existing Glue Table Name
    AllowedPattern: '[\u0020-\uD7FF\uE000-\uFFFD\uD800\uDC00-\uDBFF\uDFFF\t]*'


  ClusterSize:
    Description: Please choose the size of your EMR Cluster to meet the desired migration workload
    Type: String
    Default: Small
    AllowedValues:
      - Small
      - Medium
      - Large
      - Xlarge
    ConstraintDescription: Cluster Size must be within the allowed value


  MigrationType:
    AllowedValues:
      - New-Migration
    Description: New-Migration use CTAS [Create Table As Select] to migrate to a new Amazon S3Table Buckets namespace and table
    Type: String
    Default: New-Migration   


  RecipientEmail:
    Description: Please enter the Email address to receive Job notifications. Please remember to Confirm the Subscription
    Type: String
    MinLength: '5'
    MaxLength: '150'
    ConstraintDescription: Please enter a valid email address

  subnetIDs:
    Description: Please choose exactly two Subnets, EMR will evaluate and deploy into only one of the subnets. Please review EMR Documentation for guidance.
    Type: List<AWS::EC2::Subnet::Id>    
    MinLength: '1'
    ConstraintDescription: Please specify exactly two SubnetIDs

  KeyPair:
    Type: 'AWS::EC2::KeyPair::KeyName'
    Description: Please choose a KeyPair to enable SSH Access into EMR Primary Node.
    MinLength: '1'
    ConstraintDescription: Please specify an EC2 Keypair name


Mappings:
  PySpark:
    Script:
      s3key: resources/script/mys3tablespysparkscript.py
      csvwithversionid: restore-and-copy/csv-manifest/with-version-id/
    Parameter:
      catalogname: s3tablescatalog

  EMR:
    Cluster:
      releaselabel: emr-7.5.0
    Small:
      PrimaryInstanceCount: 1
      PrimaryInstanceType: m5.4xlarge
      PrimaryInstanceType2: m5d.4xlarge
      CoreInstanceCount: 1
      CoreInstanceType: i3.4xlarge
      CoreInstanceType2: r5d.4xlarge
      TaskInstanceCount: 1
      TaskInstanceType: i3.4xlarge
      TaskInstanceType2: r5d.4xlarge
      executorMemory: 24G
      executorCores: 4
      driverMemory: 24G
      driverCores: 4
      dynamicAllocMinExec: 2
      dynamicAllocMaxExec: 7
      executorMemoryOverhead: 8G
      driverMemoryOverhead: 2G
      driverMaxResultsSize: 4G


    Medium:
      PrimaryInstanceCount: 1
      PrimaryInstanceType: m5.4xlarge
      PrimaryInstanceType2: m5d.4xlarge
      CoreInstanceCount: 4
      CoreInstanceType: i3.4xlarge
      CoreInstanceType2: r5d.4xlarge
      TaskInstanceCount: 4
      TaskInstanceType: i3.4xlarge   
      TaskInstanceType2: r5d.4xlarge
      executorMemory: 24G
      executorCores: 4
      driverMemory: 24G
      driverCores: 4
      dynamicAllocMinExec: 8
      dynamicAllocMaxExec: 29
      executorMemoryOverhead: 8G
      driverMemoryOverhead: 2G
      driverMaxResultsSize: 4G

  
    Large:
      PrimaryInstanceCount: 1
      PrimaryInstanceType: r5.4xlarge
      PrimaryInstanceType2: i3.4xlarge
      CoreInstanceCount: 4
      CoreInstanceType: i3.4xlarge
      CoreInstanceType2: r5d.4xlarge
      TaskInstanceCount: 8
      TaskInstanceType: i3.4xlarge  
      TaskInstanceType2: r5d.4xlarge
      executorMemory: 24G
      executorCores: 3
      driverMemory: 32G
      driverCores: 4
      dynamicAllocMinExec: 12
      dynamicAllocMaxExec: 44
      executorMemoryOverhead: 8G
      driverMemoryOverhead: 6G
      driverMaxResultsSize: 12G

 
    Xlarge:
      PrimaryInstanceCount: 1
      PrimaryInstanceType: r5.4xlarge
      PrimaryInstanceType2: i3.4xlarge
      CoreInstanceCount: 8
      CoreInstanceType: i3.4xlarge
      CoreInstanceType2: r5d.4xlarge
      TaskInstanceCount: 12
      TaskInstanceType: i3.4xlarge 
      TaskInstanceType2: r5d.4xlarge
      executorMemory: 28G
      executorCores: 4
      driverMemory: 48G
      driverCores: 4
      dynamicAllocMinExec: 20
      dynamicAllocMaxExec: 74  
      executorMemoryOverhead: 8G
      driverMemoryOverhead: 6G   
      driverMaxResultsSize: 16G


  Performance:
    Parameters:
      sdkretryattempts: 10




Resources:

  Topic:
    DependsOn:
      - CheckResourceExists   
    Type: AWS::SNS::Topic
    Properties: 
      KmsMasterKeyId: alias/aws/sns    


  TopicSubscription:
    DependsOn:
      - CheckResourceExists   
    Type: AWS::SNS::Subscription
    Properties:
      Endpoint: !Ref RecipientEmail
      Protocol: email
      TopicArn: !Ref Topic




################################## Custom Resources ##############################################################

################################ CheckResourceExists ######################################################

  CheckResourceExists:
    Type: 'Custom::LambdaTrigger'
    Properties:
      ServiceToken: !GetAtt CheckResourceExistsLambdaFunction.Arn
      bucketexists: !Ref YourS3Bucket
      sourcetableexists: !Ref YourExistingGlueTable
      sourcedbexists: !Ref YourExistingGlueDatabase


  CheckResourceExistsIAMRole:
    Type: 'AWS::IAM::Role'
    Properties:  
      AssumeRolePolicyDocument:
        Version: 2012-10-17
        Statement:
          - Effect: Allow
            Principal:
              Service:
                - lambda.amazonaws.com
            Action:
              - 'sts:AssumeRole'
      Path: /
      Policies:
        - PolicyName: AWSLambdaBasicExecutionRole
          PolicyDocument:
            Version: "2012-10-17"
            Statement:
              - Action:
                  - 'logs:CreateLogGroup'
                  - 'logs:CreateLogStream'
                  - 'logs:PutLogEvents'                  
                Resource: !Sub 'arn:${AWS::Partition}:logs:${AWS::Region}:${AWS::AccountId}:log-group:*'
                Effect: Allow       
        - PolicyName: CheckBucketExistsPermissions
          PolicyDocument:
            Version: 2012-10-17
            Statement:
              - Effect: Allow
                Action:
                  - 's3:GetBucketLocation'
                Resource: !Sub arn:${AWS::Partition}:s3:::${YourS3Bucket}
        - PolicyName: CheckSourceTableExistsPermissions
          PolicyDocument:
            Version: 2012-10-17
            Statement:
              - Effect: Allow
                Action:
                  - glue:GetTable
                  - glue:GetTables
                Resource: 
                  - !Sub "arn:${AWS::Partition}:glue:${AWS::Region}:${AWS::AccountId}:table/${YourExistingGlueDatabase}/${YourExistingGlueTable}" 
                  - !Sub "arn:${AWS::Partition}:glue:${AWS::Region}:${AWS::AccountId}:database/${YourExistingGlueDatabase}"
                  - !Sub "arn:${AWS::Partition}:glue:${AWS::Region}:${AWS::AccountId}:catalog"       


  CheckResourceExistsLambdaFunction:
    Type: 'AWS::Lambda::Function'
    Properties:
      Architectures:
        - arm64
      Handler: index.lambda_handler
      Role: !GetAtt CheckResourceExistsIAMRole.Arn
      Runtime: python3.12
      Timeout: 150
      MemorySize: 128
      Code:
        ZipFile: |
            import json
            import cfnresponse
            import logging
            import os
            import boto3
            from botocore.exceptions import ClientError
            from botocore.client import Config

            # Enable debugging for troubleshooting
            # boto3.set_stream_logger("")


            # Set up logging
            logger = logging.getLogger(__name__)
            logger.setLevel('INFO')


            # Define Environmental Variables
            my_region = str(os.environ['AWS_REGION'])


            # Set SDK paramters
            config = Config(retries = {'max_attempts': 5})

            # Set variables
            # Set Service Parameters
            s3Client = boto3.client('s3', config=config, region_name=my_region)
            glueClient = boto3.client('glue', region_name=my_region)


            def get_table(db_name, tbl_name):
                logger.info(f"Checking if Source Glue Table Exists")
                try:
                    check_table = glueClient.get_table(
                        DatabaseName=db_name,
                        Name=tbl_name,
                    )
                except Exception as e:
                    logger.error(e)
                    raise e
                else:
                    logger.info(check_table.get('Table').get('Name'))
                    logger.info(f"Table {tbl_name} exists!")
                    return check_table



            def check_bucket_exists(bucket):
                logger.info(f"Checking if Source Bucket Exists")
                try:
                    check_bucket = s3Client.get_bucket_location(
                        Bucket=bucket,
                    )
                except ClientError as e:
                    logger.error(e)
                    raise
                else:
                    logger.info(f"Bucket {bucket}, exists, proceeding with deployment ...")
                    return check_bucket            


            def lambda_handler(event, context):
              # Define Environmental Variables
              s3Bucket  = event.get('ResourceProperties').get('bucketexists')
              gluedb = event.get('ResourceProperties').get('sourcedbexists')
              gluetbl = event.get('ResourceProperties').get('sourcetableexists')

              logger.info(f'Event detail is: {event}')

              if event.get('RequestType') == 'Create':
                # logger.info(event)
                try:
                  logger.info("Stack event is Create, checking specified Source S3 Bucket and Source Glue Table exists...")
                  if s3Bucket:
                    check_bucket_exists(s3Bucket)
                  get_table(gluedb, gluetbl)  
                  responseData = {}
                  responseData['message'] = "Successful"
                  logger.info(f"Sending Invocation Response {responseData['message']} to Cloudformation Service")
                  cfnresponse.send(event, context, cfnresponse.SUCCESS, responseData)
                except Exception as e:
                  logger.error(e)
                  responseData = {}
                  responseData['message'] = str(e)
                  failure_reason = str(e) 
                  logger.info(f"Sending Invocation Response {responseData['message']} to Cloudformation Service")
                  cfnresponse.send(event, context, cfnresponse.FAILED, responseData, reason=failure_reason)


              elif event.get('RequestType') == 'Delete' or event.get('RequestType') == 'Update':
                logger.info(event)
                try:
                  logger.info(f"Stack event is Delete or Update, nothing to do....")
                  responseData = {}
                  responseData['message'] = "Completed"
                  logger.info(f"Sending Invocation Response {responseData['message']} to Cloudformation Service")
                  cfnresponse.send(event, context, cfnresponse.SUCCESS, responseData)
                except Exception as e:
                  logger.error(e)
                  responseData = {}
                  responseData['message'] = str(e)
                  logger.info(f"Sending Invocation Response {responseData['message']} to Cloudformation Service")
                  cfnresponse.send(event, context, cfnresponse.FAILED, responseData)                  



################################################ Code Ends ####################################################


  EMRLogS3Bucket:
    DependsOn:
      - CheckResourceExists    
    Type: 'AWS::S3::Bucket'
    DeletionPolicy: Retain
    UpdateReplacePolicy: Retain
    Properties:
      BucketEncryption:
        ServerSideEncryptionConfiguration:
          - ServerSideEncryptionByDefault:
              SSEAlgorithm: AES256
      LifecycleConfiguration:
        Rules:
          - Id: ExpirationRule
            Prefix: logs/
            Status: Enabled
            ExpirationInDays: 180
            NoncurrentVersionExpiration:
                NoncurrentDays: 3
          - Id: delete-incomplete-mpu
            Status: Enabled
            AbortIncompleteMultipartUpload:
              DaysAfterInitiation: 1       

############################################################ Upload PySpark Script to S3 Bucket ###################################################


  UploadScriptCustomResource:
    DependsOn:
      - CheckResourceExists   
    Type: Custom::EmptyS3Bucket
    Properties:
      ServiceToken: !GetAtt UploadScriptFunction.Arn

  UploadScriptFunctionIAMRole:
    DependsOn:
      - CheckResourceExists 
    Type: AWS::IAM::Role
    Properties:
      AssumeRolePolicyDocument:
        Version: "2012-10-17"
        Statement:
          - Effect: "Allow"
            Principal:
              Service:
                - "lambda.amazonaws.com"
            Action:
              - "sts:AssumeRole"
      Path: '/'
      Policies:
        - PolicyName: AWSLambdaBasicExecutionRole
          PolicyDocument:
            Version: "2012-10-17"
            Statement:
              - Action:
                  - 'logs:CreateLogGroup'
                  - 'logs:CreateLogStream'
                  - 'logs:PutLogEvents'                  
                Resource: !Sub 'arn:${AWS::Partition}:logs:${AWS::Region}:${AWS::AccountId}:log-group:*'
                Effect: Allow        
        - PolicyName: WriteObjectPolicy
          PolicyDocument:
            Version: '2012-10-17'
            Statement:
              - Effect: Allow
                Action:
                  - 's3:PutObject*'
                  - 's3:List*'
                  - 's3:GetObject*'
                Resource:
                  - !Sub arn:${AWS::Partition}:s3:::${EMRLogS3Bucket}/*      
                  - !Sub arn:${AWS::Partition}:s3:::${EMRLogS3Bucket}                



  UploadScriptFunction:
    DependsOn:
      - CheckResourceExists 
    Type: 'AWS::Lambda::Function'
    Properties:
      Architectures:
        - arm64
      Environment:
        Variables:
          asset1: !FindInMap [ PySpark, Script, s3key ] 
          my_account_id: !Sub ${AWS::AccountId}
          s3BuckettoDownload: !Ref EMRLogS3Bucket
          max_attempts: !FindInMap [ Performance, Parameters, sdkretryattempts ] 
          asset1_key: !FindInMap [ PySpark, Script, s3key ]
      Description: Downloads Function Source Code from Github to S3
      MemorySize: 384
      Runtime: python3.12
      Handler: index.lambda_handler
      Role: !GetAtt UploadScriptFunctionIAMRole.Arn
      Timeout: 360
      Code:
        ZipFile: |
            import cfnresponse
            import boto3
            import io
            import json
            import logging
            import uuid
            import os
            from botocore.client import Config
            from botocore.exceptions import ClientError
            from boto3.s3.transfer import TransferConfig

            # Enable Debug logging
            boto3.set_stream_logger('')

            # Setup Logging
            logger = logging.getLogger(__name__)
            logger.setLevel('INFO')

            # Define Environmental Variables
            my_asset1_key = str(os.environ['asset1_key'])
            my_bucket = str(os.environ['s3BuckettoDownload'])
            my_max_attempts = int(os.environ['max_attempts'])
            my_region = str(os.environ['AWS_REGION'])


            # Set and Declare Configuration Parameters
            config = Config(retries={'max_attempts': my_max_attempts})

            # Set Service Clients
            s3 = boto3.resource('s3', config=config)


            # Upload PySpark Script to Solution S3 Bucket
            def stream_to_s3(bucket, key, body):
                logger.info(f'Starting PySpark Script upload to the S3 Bucket: s3://{bucket}/{key}')
                try:
                    upload_to_s3 = s3.Object(bucket, key).put(Body=body)
                except Exception as e:
                    logger.error(e)
                else:
                    logger.info(f'Object successfully uploaded to s3://{bucket}/{key}')


            # Define PySpark Script to Upload as blob
            my_blob = f'''
            import sys
            import argparse
            from pyspark.sql import SparkSession
            from pyspark import SparkConf
            import logging

            # Setup Logging
            logger = logging.getLogger(__name__)
            logger.setLevel('INFO')

            # Import Sys Arguments
            parser = argparse.ArgumentParser()
            parser.add_argument('--data_migration_type', help="Data Migration type new or insert/update.")
            parser.add_argument('--data_source_bucket', help="Source data S3 bucket name.")
            parser.add_argument('--data_source_db', help="Source data Glue Database name.")
            parser.add_argument('--data_source_tbl', help="Source data Glue Table name.")
            parser.add_argument('--data_destination_s3tables_arn', help="Destination S3 Table ARN.")
            parser.add_argument('--data_destination_catalog', help="Destination S3 Tables Namespace/Database.")
            parser.add_argument('--data_destination_s3tables_namespace', help="Destination S3 Tables Namespace/Database.")
            parser.add_argument('--data_destination_s3tables_tbl', help="Destination S3 Tables Table name .")
            parser.add_argument('--data_destination_s3tables_partitions', help="Destination S3 Tables Table Partitions .")


            # Initiate ARGS
            args = parser.parse_args()

            # Now define the variables
            data_migration_type = args.data_migration_type
            data_source_bucket = args.data_source_bucket
            data_source_db = args.data_source_db
            data_source_tbl = args.data_source_tbl
            data_destination_catalog = args.data_destination_catalog
            data_destination_s3tables_arn = args.data_destination_s3tables_arn
            data_destination_s3tables_namespace = args.data_destination_s3tables_namespace
            data_destination_s3tables_tbl = args.data_destination_s3tables_tbl
            data_destination_s3tables_partitions = args.data_destination_s3tables_partitions

            # Create Spark Configuration Set
            conf = SparkConf() \
                .set("spark.sql.catalogImplementation", "hive") \
                .set("mapreduce.input.fileinputformat.input.dir.recursive", "true") \
                .set(f"spark.sql.catalog.{{data_destination_catalog}}", "org.apache.iceberg.spark.SparkCatalog") \
                .set(f"spark.sql.extensions", "org.apache.iceberg.spark.extensions.IcebergSparkSessionExtensions") \
                .set(f"spark.sql.catalog.{{data_destination_catalog}}.catalog-impl", "software.amazon.s3tables.iceberg.S3TablesCatalog") \
                .set(f"spark.sql.catalog.{{data_destination_catalog}}.io-impl", "org.apache.iceberg.aws.s3.S3FileIO") \
                .set(f"spark.sql.catalog.{{data_destination_catalog}}.warehouse", data_destination_s3tables_arn) 



            # Initiate PySpark Session
            spark = SparkSession.builder.appName("MyMigrationApp").config(conf=conf).getOrCreate()

            # Function for creating a New NameSpace in Amazon S3 Table Bucket
            def create_namespace(catalog, dst_db): 
                # Create a NameSpace in S3 Table Buckets first
                try:
                    # Create the Namespace first
                    sql_query_namespace = f"""
                    CREATE NAMESPACE IF NOT EXISTS
                    `{{catalog}}`.`{{dst_db}}`
                    """        
                    # Now run the query
                    spark_sql_query_namespace = spark.sql(sql_query_namespace)                    
                except Exception as e:
                    print(e)
                    raise e                       



            # Function for performing CTAS - CREATE TABLE AS SELECT into a new destination Database/Table - creates a new DB/Table
            def ctas_action(catalog, src_db, src_tbl, dst_db, dst_tbl, dst_partitions):
                """
                Use CTAS to load data from source to S3 Tables Bucket
                :param:
                """
                print(f"Echo parameters catalog={{catalog}}, src_db={{src_db}}, src_tbl={{src_tbl}}, dst_db={{dst_db}}, dst_tbl={{dst_tbl}}")
                # We need to create the namespace/database first, so calling the namespace function
                print(f"Creating the namespace {{dst_db}} first if it does not already exist....")
                create_namespace(catalog, dst_db)
                print(f"Creating the namespace {{dst_db}} is successful proceeding to CTAS, please hold...")

                try:
                    # Do a CTAS to migrate table data from source Table to S3 Tables Bucket
                    # If destination partition is provided, them include partition info in CTAS query
                    sql_query_d = ''
                    # Check the provided partition name and value for the destination Table
                    if dst_partitions:
                        if dst_partitions == "NotApplicable":
                            sql_query_d = f"""
                            CREATE TABLE IF NOT EXISTS
                            `{{catalog}}`.`{{dst_db}}`.`{{dst_tbl}}`
                            USING iceberg
                            AS SELECT * FROM `{{src_db}}`.`{{src_tbl}}` 
                            """
                        else:
                            sql_query_d = f"""
                            CREATE TABLE IF NOT EXISTS
                            `{{catalog}}`.`{{dst_db}}`.`{{dst_tbl}}`
                            USING iceberg
                            PARTITIONED BY {{dst_partitions}}
                            AS SELECT * FROM `{{src_db}}`.`{{src_tbl}}` 
                            """

                    # Run the CTAS SQL query
                    spark_sql_query_d = spark.sql(sql_query_d)
                except Exception as e:
                    print(e)
                    raise e
                else:
                    print(f"Create Table as Select (CTAS) completed....")


            # Function for performing a querying on a Table
            def query_table_data(catalog, db, tbl):
                """
                Check that we can access the Table data
                :param:
                """
                # Handle query with or without catalog name provided
                if catalog:
                    sql_query_data = f"""SELECT * 
                    FROM `{{catalog}}`.`{{db}}`.`{{tbl}}`
                    limit 10
                    """
                else:
                    sql_query_data = f"""SELECT * 
                    FROM `{{db}}`.`{{tbl}}`
                    limit 10
                    """

                try:
                    # Run Spark SQL Query
                    spark_sql_query_data = spark.sql(sql_query_data)
                except Exception as e:
                    print(e)
                    raise e
                else:
                    return spark_sql_query_data


            # Main workflow Function, calls other functions as needed
            def initiate_workflow():
                """
                Initiate Migration Workflow

                """
                try:
                    # First let's query the source table
                    print(f"Let do a test query of the source table {{data_source_db}}.{{data_source_tbl}} to see if we can perform a successful query")
                    query_table_data(None, data_source_db, data_source_tbl)
                    print(f"Test query of the source table {{data_source_db}}.{{data_source_tbl}} is successful proceeding to main task")
                    # Choose the CTAS option to create new Amazon S3 Table Bucket destination NameSpace and Table
                    if data_migration_type == 'New-Migration':
                        print(f"We are performing a new migration, so will use CTAS to create a new table and load data")
                        ctas_action(data_destination_catalog, data_source_db, data_source_tbl, data_destination_s3tables_namespace,
                                    data_destination_s3tables_tbl, data_destination_s3tables_partitions
                                    )

                    # Now we are done with CTAS, let's perform some verifications on the destination Table
                    # Let's query the destination table
                    print(f"Let do a test query of the destination table {{data_destination_s3tables_namespace}}.{{data_destination_s3tables_tbl}} to see if we can perform a successful query")
                    query_table_data(data_destination_catalog, data_destination_s3tables_namespace, data_destination_s3tables_tbl)
                    print(f"Test query of the destination table {{data_destination_s3tables_namespace}}.{{data_destination_s3tables_tbl}} is successful!! ")
                    """ Migration and verification was successful!"""

                except Exception as e:
                    print(e)
                    sys.exit(1)
                else:
                    # Finalize Job
                    print("Successful Job completion")



            if __name__ == "__main__":
                # Start the Main Task
                initiate_workflow()
            '''
            # End F String and PySpark Blob

            # Initiating Main Function
            def lambda_handler(event, context):
                logger.info(f'Event detail is: {event}')
                # Start Cloudformation Invocation #
                if event.get('RequestType') == 'Create':
                    # logger.info(event)
                    try:
                        logger.info("Stack event is Create or Update, Uploading PySpark to S3 Bucket...")
                        # Now upload the Script to the Solution Amazon S3 Bucket!.
                        stream_to_s3(my_bucket, my_asset1_key, my_blob)

                        responseData = {}
                        responseData['message'] = "Successful"
                        logger.info(f"Sending Invocation Response {responseData['message']} to Cloudformation Service")
                        cfnresponse.send(event, context, cfnresponse.SUCCESS, responseData)
                    except Exception as e:
                        logger.error(e)
                        responseData = {}
                        responseData['message'] = str(e)
                        failure_reason = str(e)
                        logger.info(f"Sending Invocation Response {responseData['message']} to Cloudformation Service")
                        cfnresponse.send(event, context, cfnresponse.FAILED, responseData, reason=failure_reason)

                else:
                    logger.info(f"Stack event is Update or Delete, nothing to do....")
                    responseData = {}
                    responseData['message'] = "Completed"
                    logger.info(f"Sending Invocation Response {responseData['message']} to Cloudformation Service")
                    cfnresponse.send(event, context, cfnresponse.SUCCESS, responseData)

################################################ Code Ends #############################################################


################################################### EMR EC2 Cluster ####################################################

##############################  Start State Machine ################################################


  EMREC2StateMachineExecutionRole:
    DependsOn:
      - CheckResourceExists   
    Type: AWS::IAM::Role
    Properties:
      AssumeRolePolicyDocument:
        Version: 2012-10-17
        Statement:
          - Effect: Allow
            Principal:
              Service: states.amazonaws.com
            Action: sts:AssumeRole
      Path: /
      Policies:
        - PolicyName: StatesExecutionPolicy
          PolicyDocument:
            Version: 2012-10-17
            Statement:
              - Effect: Allow
                Action:
                  - iam:PassRole
                Resource:
                  - !GetAtt EMRServiceRole.Arn
                  - !GetAtt EMREc2Role.Arn
              - Effect: Allow
                Action:
                  - elasticmapreduce:RunJobFlow
                  - elasticmapreduce:TerminateJobFlows
                  - elasticmapreduce:DescribeCluster
                  - elasticmapreduce:AddJobFlowSteps
                  - elasticmapreduce:DescribeStep
                  - elasticmapreduce:AddTags
                Resource: !Sub arn:${AWS::Partition}:elasticmapreduce:${AWS::Region}:${AWS::AccountId}:cluster/*
        - PolicyName: AllowServiceLinkedRole      
          PolicyDocument:
            Statement:
              - Action:
                  - iam:CreateServiceLinkedRole
                  - iam:PutRolePolicy
                Effect: Allow
                Resource: !Sub arn:${AWS::Partition}:iam::*:role/aws-service-role/elasticmapreduce.amazonaws.com*/AWSServiceRoleForEMRCleanup*
                Condition:
                  StringLike:
                    iam:AWSServiceName:
                    - elasticmapreduce.amazonaws.com
                    - elasticmapreduce.amazonaws.com.cn
            Version: 2012-10-17
        - PolicyName: SNSPublishPolicy
          PolicyDocument:
            Version: '2012-10-17'
            Statement:
              - Effect: Allow
                Action:
                  - 'sns:Publish'
                Resource:
                  - !Ref Topic
     


  EMREC2StateMachine:
    DependsOn:
      - CheckResourceExists   
    Type: AWS::StepFunctions::StateMachine
    Properties:
      RoleArn: !GetAtt [ EMREC2StateMachineExecutionRole, Arn ]
      DefinitionString:
        !Sub
          - |-
            {
              "Comment": "Amazon EMR for Migrating to Amazon S3 Tables Bucket",
              "StartAt": "Create an EMR cluster",
              "States": {
                "Create an EMR cluster": {
                  "Type": "Task",
                  "Resource": "arn:${AWS::Partition}:states:::elasticmapreduce:createCluster.sync",
                  "Parameters": {
                    "Name": "MigratetoS3Tables",
                    "VisibleToAllUsers": true,
                    "ReleaseLabel": "${EMRReleaseLabel}",
                    "Tags": [{"Key":"for-use-with-amazon-emr-managed-policies", "Value": "true"}],
                    "Applications": [
                      {
                        "Name": "Hive"
                      },
                      {
                        "Name": "Hadoop"
                      },
                      {
                        "Name": "Livy"
                      },                      
                      {
                        "Name": "Spark"
                      }
                    ],
                    "ServiceRole": "${EMRServiceRole}",
                    "JobFlowRole": "${EMREc2InstanceProfile}",
                    "Configurations": [
                      {
                        "Classification": "hive-site",
                        "Properties": {
                          "hive.metastore.client.factory.class": "com.amazonaws.glue.catalog.metastore.AWSGlueDataCatalogHiveClientFactory"
                        }
                      },
                      {
                        "Classification": "spark-hive-site",
                        "Properties": {
                          "hive.metastore.client.factory.class": "com.amazonaws.glue.catalog.metastore.AWSGlueDataCatalogHiveClientFactory"
                        }
                      },                      
                      {
                        "Classification": "iceberg-defaults",
                        "Properties": {
                          "iceberg.enabled": "true"
                        }
                      },
                      {
                        "Classification": "emrfs-site",
                        "Properties": {
                          "fs.s3.aimd.enabled": "true"
                        }
                      },
                      {
                        "Classification": "spark-defaults",
                        "Properties": {
                          "spark.executor.memory": "${executorMemory}",
                          "spark.executor.cores": "${executorCores}",
                          "spark.driver.memory": "${driverMemory}",
                          "spark.driver.cores": "${driverCores}",
                          "spark.dynamicAllocation.minExecutors": "${dynamicAllocMinExec}",
                          "spark.dynamicAllocation.maxExecutors": "${dynamicAllocMaxExec}",
                          "spark.driver.memoryOverhead": "${driverMemoryOverhead}",
                          "spark.executor.memoryOverhead": "${executorMemoryOverhead}",
                          "spark.driver.maxResultSize": "${driverMaxResultsSize}"
                        }
                      }                      
                    ],
                    "LogUri": "s3://${EMRLogS3Bucket}/logs/",
                    "Instances": {
                      "KeepJobFlowAliveWhenNoSteps": true,
                      "Ec2SubnetIds": ["${SubnetIDOne}", "${SubnetIDTwo}"],
                      "Ec2KeyName": "${InstanceKeyPair}",
                      "InstanceFleets": [
                        {
                          "Name": "MyPrimaryFleet",
                          "InstanceFleetType": "MASTER",
                          "TargetOnDemandCapacity": ${EMRPryInstanceCount},
                          "InstanceTypeConfigs": [
                            {
                              "InstanceType": "${EMRPryInstanceType}",
                              "WeightedCapacity": 1,
                              "EbsConfiguration": {
                                  "EbsBlockDeviceConfigs": [
                                  {
                                      "VolumeSpecification": {
                                          "VolumeType": "gp3",
                                          "SizeInGB": 32
                                      },
                                      "VolumesPerInstance":2
                                  }
                                ]
                              }                               
                            },
                            {
                              "InstanceType": "${EMRPryInstanceType2}",
                              "WeightedCapacity": 1,
                              "EbsConfiguration": {
                                  "EbsBlockDeviceConfigs": [
                                  {
                                      "VolumeSpecification": {
                                          "VolumeType": "gp3",
                                          "SizeInGB": 32
                                      },
                                      "VolumesPerInstance":2
                                  }
                                ]
                              }                               
                            }                            
                          ]
                        },
                        {
                          "Name": "MyCoreFleet",
                          "InstanceFleetType": "CORE",
                          "TargetOnDemandCapacity": ${EMRCoreInstanceCount},
                          "InstanceTypeConfigs": [
                            {
                              "InstanceType": "${EMRCoreInstanceType}",
                              "WeightedCapacity": 1,
                              "EbsConfiguration": {
                                  "EbsBlockDeviceConfigs": [
                                  {
                                      "VolumeSpecification": {
                                          "VolumeType": "gp3",
                                          "SizeInGB": 32
                                      },
                                      "VolumesPerInstance":4
                                  }
                                ]
                              }                               
                            },
                            {
                              "InstanceType": "${EMRCoreInstanceType2}",
                              "WeightedCapacity": 1,
                              "EbsConfiguration": {
                                  "EbsBlockDeviceConfigs": [
                                  {
                                      "VolumeSpecification": {
                                          "VolumeType": "gp3",
                                          "SizeInGB": 32
                                      },
                                      "VolumesPerInstance":4
                                  }
                                ]
                              }                               
                            }                            
                          ]
                        },
                        {
                          "Name": "MyTaskFleet",
                          "InstanceFleetType": "TASK",
                          "TargetOnDemandCapacity": ${EMRTaskInstanceCount},
                          "InstanceTypeConfigs": [
                            {
                              "InstanceType": "${EMRTaskInstanceType}",
                              "WeightedCapacity": 1,
                              "EbsConfiguration": {
                                  "EbsBlockDeviceConfigs": [
                                  {
                                      "VolumeSpecification": {
                                          "VolumeType": "gp3",
                                          "SizeInGB": 32
                                      },
                                      "VolumesPerInstance":3
                                  }
                                ]
                              }                               
                            },
                            {
                              "InstanceType": "${EMRTaskInstanceType2}",
                              "WeightedCapacity": 1,
                              "EbsConfiguration": {
                                  "EbsBlockDeviceConfigs": [
                                  {
                                      "VolumeSpecification": {
                                          "VolumeType": "gp3",
                                          "SizeInGB": 32
                                      },
                                      "VolumesPerInstance":3
                                  }
                                ]
                              }                               
                            }                            
                          ]
                        }                          
                      ]
                    }
                  },
                  "ResultPath": "$.cluster",
                  "Next": "Notify EMR Creation",
                  "Retry": [
                    {
                      "ErrorEquals": [
                        "States.ALL"
                      ],
                      "BackoffRate": 2,
                      "IntervalSeconds": 1,
                      "MaxAttempts": 3,
                      "JitterStrategy": "FULL"
                    }
                  ],
                  "Catch": [
                    {
                      "ErrorEquals": [
                        "States.ALL"
                      ],
                      "Comment": "CatchError",
                      "Next": "ClusterFailed",
                      "ResultPath": "$.error"
                    }
                  ]
                },
                "NotifyFailedRetry": {
                  "Type": "Task",
                  "Resource": "arn:${AWS::Partition}:states:::sns:publish",
                  "Parameters": {
                    "TopicArn": "${snspublish_TopicArn_a2a28236}",
                    "Message": {
                      "Subject": "Automated Migration to Amazon S3 Tables Bucket",
                      "Message.$": "$.error"
                    }
                  },
                  "Next": "Terminate Cluster",
                  "ResultPath": null
                },
                "Notify EMR Creation": {
                  "Type": "Task",
                  "Resource": "arn:${AWS::Partition}:states:::sns:publish",
                  "Parameters": {
                    "TopicArn": "${snspublish_TopicArn_a2a28236}",
                    "Message": {
                      "Subject": "Automated Migration to Amazon S3 Tables Bucket",
                      "Status": "EMR Cluster Creation",
                      "Message": "EMR EC2 Cluster Successfully Created!- Initiating Table Migration Step"
                    }
                  },
                  "Next": "Initiate Table Migration",
                  "Retry": [
                    {
                      "ErrorEquals": [
                        "States.ALL"
                      ],
                      "BackoffRate": 2,
                      "IntervalSeconds": 1,
                      "MaxAttempts": 3,
                      "JitterStrategy": "FULL"
                    }
                  ],
                  "ResultPath": null
                },
                "Initiate Table Migration": {
                  "Type": "Task",
                  "Resource": "arn:${AWS::Partition}:states:::elasticmapreduce:addStep.sync",
                  "Parameters": {
                    "ClusterId.$": "$.cluster.ClusterId",
                    "Step": {
                      "Name": "My first EMR step",
                      "ActionOnFailure": "CONTINUE",
                      "HadoopJarStep": {
                        "Jar": "command-runner.jar",
                        "Args": [
                          "spark-submit",
                          "--master",
                          "yarn",
                          "--conf",
                          "spark.jars.packages=org.apache.iceberg:iceberg-spark-runtime-3.5_2.12:1.6.1,software.amazon.s3tables:s3-tables-catalog-for-iceberg-runtime:0.1.3",                          
                          "${PySparkScriptURI}",
                          "--data_migration_type",
                          "${DataMigrationType}",
                          "--data_source_bucket",
                          "${DataSourceS3Bucket}",
                          "--data_source_db",
                          "${DataSourceGlueDatabase}",
                          "--data_source_tbl",
                          "${DataSourceGlueTable}",
                          "--data_destination_catalog",
                          "${DataDestinationCatalog}",
                          "--data_destination_s3tables_arn",
                          "${DataDestinationS3TablesArn}",
                          "--data_destination_s3tables_namespace",
                          "${DataDestinationS3TablesNamespace}",
                          "--data_destination_s3tables_tbl",
                          "${DataDestinationS3TablesTables}",
                          "--data_destination_s3tables_partitions",
                          "${DataDestinationS3TablesPartitions}"                                                                       
                        ]
                      }
                    }
                  },
                  "Retry": [
                    {
                      "ErrorEquals": [
                        "States.ALL"
                      ],
                      "IntervalSeconds": 1,
                      "MaxAttempts": 3,
                      "BackoffRate": 2
                    }
                  ],
                  "ResultPath": "$.firstStep",
                  "Next": "Notify Migration Complete",
                  "Catch": [
                    {
                      "ErrorEquals": [
                        "States.ALL"
                      ],
                      "Next": "NotifyFailedRetry",
                      "ResultPath": "$.error"
                    }
                  ]
                },
                "Notify Migration Complete": {
                  "Type": "Task",
                  "Resource": "arn:${AWS::Partition}:states:::sns:publish",
                  "Parameters": {
                    "TopicArn": "${snspublish_TopicArn_a2a28236}",
                    "Message": {
                      "Subject": "Automated Migration to Amazon S3 Tables Bucket",
                      "Status": "Task Complete",
                      "Message": "EMR Table Migration to S3 Tables completed successfully! Initiating Cluster Termination"
                    }
                  },
                  "Next": "Terminate Cluster",
                  "Retry": [
                    {
                      "ErrorEquals": [
                        "States.ALL"
                      ],
                      "BackoffRate": 2,
                      "IntervalSeconds": 1,
                      "MaxAttempts": 3,
                      "JitterStrategy": "FULL"
                    }
                  ],
                  "ResultPath": null
                },
                "Terminate Cluster": {
                  "Type": "Task",
                  "Resource": "arn:${AWS::Partition}:states:::elasticmapreduce:terminateCluster",
                  "Parameters": {
                    "ClusterId.$": "$.cluster.ClusterId"
                  },
                  "End": true,
                  "Retry": [
                    {
                      "ErrorEquals": [
                        "States.ALL"
                      ],
                      "BackoffRate": 2,
                      "IntervalSeconds": 1,
                      "MaxAttempts": 3,
                      "JitterStrategy": "FULL",
                      "MaxDelaySeconds": 2
                    }
                  ],
                  "Catch": [
                    {
                      "ErrorEquals": [
                        "States.ALL"
                      ],
                      "Next": "ClusterFailed",
                      "ResultPath": "$.error"
                    }
                  ]
                },
                "ClusterFailed": {
                  "Type": "Task",
                  "Resource": "arn:${AWS::Partition}:states:::sns:publish",
                  "Parameters": {
                    "TopicArn": "${snspublish_TopicArn_a2a28236}",
                    "Message": {
                      "Subject": "Automated Migration to Amazon S3 Tables Bucket",
                      "Message.$": "$.error"
                    }
                  },
                  "End": true
                }
              }
            }
          - EMRServiceRole: !Ref EMRServiceRole
            EMREc2InstanceProfile: !Ref EMREc2InstanceProfile
            EMRLogS3Bucket: !Ref EMRLogS3Bucket
            snspublish_TopicArn_a2a28236: !Ref Topic
            DataSourceS3Bucket: !Ref YourS3Bucket
            DataSourceGlueTable: !Ref YourExistingGlueTable
            DataSourceGlueDatabase: !Ref YourExistingGlueDatabase
            DataDestinationCatalog: !FindInMap [ PySpark, Parameter, catalogname ]
            DataDestinationS3TablesArn: !Ref S3TableBucket
            DataDestinationS3TablesNamespace: !Ref S3TableBucketNamespace
            DataDestinationS3TablesTables: !Ref S3TableBucketTables
            PySparkScriptURI: !Join ['', ['s3://', !Ref EMRLogS3Bucket, '/', !FindInMap [ PySpark, Script, s3key ] ]]
            DataMigrationType: !Ref MigrationType
            DataDestinationS3TablesPartitions: !Ref S3TableBucketTablesPartitions
            EMRReleaseLabel: !FindInMap [ EMR, Cluster, releaselabel ]
            EMRPryInstanceType: !FindInMap [EMR, !Ref ClusterSize, PrimaryInstanceType]
            EMRPryInstanceType2: !FindInMap [EMR, !Ref ClusterSize, PrimaryInstanceType2]
            EMRPryInstanceCount: !FindInMap [EMR, !Ref ClusterSize, PrimaryInstanceCount]
            EMRCoreInstanceType: !FindInMap [EMR, !Ref ClusterSize, CoreInstanceType]
            EMRCoreInstanceType2: !FindInMap [EMR, !Ref ClusterSize, CoreInstanceType2]
            EMRCoreInstanceCount: !FindInMap [EMR, !Ref ClusterSize, CoreInstanceCount]
            EMRTaskInstanceType: !FindInMap [EMR, !Ref ClusterSize, TaskInstanceType]
            EMRTaskInstanceType2: !FindInMap [EMR, !Ref ClusterSize, TaskInstanceType2]
            EMRTaskInstanceCount: !FindInMap [EMR, !Ref ClusterSize, TaskInstanceCount]      
            executorMemory: !FindInMap [EMR, !Ref ClusterSize, executorMemory]  
            executorCores: !FindInMap [EMR, !Ref ClusterSize, executorCores] 
            driverMemory: !FindInMap [EMR, !Ref ClusterSize, driverMemory] 
            driverCores: !FindInMap [EMR, !Ref ClusterSize, driverCores] 
            dynamicAllocMinExec: !FindInMap [EMR, !Ref ClusterSize, dynamicAllocMinExec] 
            dynamicAllocMaxExec: !FindInMap [EMR, !Ref ClusterSize, dynamicAllocMaxExec]   
            driverMemoryOverhead: !FindInMap [EMR, !Ref ClusterSize, driverMemoryOverhead]  
            executorMemoryOverhead: !FindInMap [EMR, !Ref ClusterSize, executorMemoryOverhead]      
            driverMaxResultsSize: !FindInMap [EMR, !Ref ClusterSize, driverMaxResultsSize]     
            InstanceKeyPair: !Ref KeyPair
            SubnetIDOne: !Select [0, !Ref subnetIDs]
            SubnetIDTwo: !Select [1, !Ref subnetIDs]                            




  EMREc2InstanceProfile:
    DependsOn:
      - CheckResourceExists   
    Type: AWS::IAM::InstanceProfile
    Properties:
      Path: /
      Roles:
        - Ref: EMREc2Role

  EMREc2Role:
    DependsOn:
      - CheckResourceExists   
    Type: AWS::IAM::Role
    Properties:
      AssumeRolePolicyDocument:
        Version: 2008-10-17
        Statement:
          - Effect: Allow
            Principal:
              Service: !Sub ec2.${AWS::URLSuffix}
            Action: sts:AssumeRole
      Policies:
        - PolicyName: AccessSolutionBucket
          PolicyDocument:
            Statement:
              - Action:
                  - s3:Delete*
                  - s3:List*
                  - s3:PutObject*
                  - s3:GetObject*
                  - s3:GetBucketLocation
                Effect: Allow
                Resource: 
                  - !Sub arn:${AWS::Partition}:s3:::${EMRLogS3Bucket}/*
                  - !Sub arn:${AWS::Partition}:s3:::${EMRLogS3Bucket}
            Version: 2012-10-17
        - PolicyName: AccessDataSourceBucket
          PolicyDocument:
            Statement:
              - Action:
                  - s3:ListBucket
                  - s3:ListBucketVersions
                  - s3:GetObject
                  - s3:GetObjectVersion
                  - s3:GetBucketLocation
                Effect: Allow
                Resource: 
                  - !Sub arn:${AWS::Partition}:s3:::${YourS3Bucket}/*
                  - !Sub arn:${AWS::Partition}:s3:::${YourS3Bucket}
            Version: 2012-10-17
        - PolicyName: WritetoDestinationS3TablesBucket
          PolicyDocument:
            Statement:
              - Action:
                  - s3tables:CreateTable
                  - s3tables:PutTableData
                  - s3tables:GetTableData
                  - s3tables:GetTableMetadataLocation
                  - s3tables:UpdateTableMetadataLocation
                  - s3tables:GetNamespace
                  - s3tables:CreateNamespace
                Effect: Allow
                Resource: 
                  - !Ref S3TableBucket
                  - !Sub ${S3TableBucket}/table/*
            Version: 2012-10-17                                                            
        - PolicyName: AccessDataSourceDBandTable
          PolicyDocument:
            Version: 2012-10-17
            Statement:
              - Action:
                  - glue:GetDatabase
                  - glue:GetDatabases
                  - glue:GetPartition
                  - glue:GetTables
                  - glue:GetPartitions
                  - glue:GetTable
                Effect: Allow
                Resource: 
                  - !Sub "arn:${AWS::Partition}:glue:${AWS::Region}:${AWS::AccountId}:table/${YourExistingGlueTable}/*"
                  - !Sub "arn:${AWS::Partition}:glue:${AWS::Region}:${AWS::AccountId}:table/${YourExistingGlueTable}"                  
                  - !Sub "arn:${AWS::Partition}:glue:${AWS::Region}:${AWS::AccountId}:table/${YourExistingGlueDatabase}/*"
                  - !Sub "arn:${AWS::Partition}:glue:${AWS::Region}:${AWS::AccountId}:database/${YourExistingGlueDatabase}"
                  - !Sub "arn:${AWS::Partition}:glue:${AWS::Region}:${AWS::AccountId}:database/${YourExistingGlueDatabase}/*"
                  - !Sub "arn:${AWS::Partition}:glue:${AWS::Region}:${AWS::AccountId}:database/${YourExistingGlueTable}/*"
                  - !Sub "arn:${AWS::Partition}:glue:${AWS::Region}:${AWS::AccountId}:database/${YourExistingGlueTable}"
                  - !Sub "arn:${AWS::Partition}:glue:${AWS::Region}:${AWS::AccountId}:catalog"                                       
        - PolicyName: ReadGlueCatalogandDB
          PolicyDocument:
            Version: 2012-10-17
            Statement:
              - Action:
                  - glue:GetDatabases
                Effect: Allow
                Resource: 
                  - !Sub "arn:${AWS::Partition}:glue:${AWS::Region}:${AWS::AccountId}:catalog"      
                  - !Sub "arn:${AWS::Partition}:glue:${AWS::Region}:${AWS::AccountId}:database*"
        - PolicyName: ReadDefaultDB
          PolicyDocument:
            Version: 2012-10-17
            Statement:
              - Action:
                  - glue:GetDatabase
                Effect: Allow
                Resource:   
                  - !Sub "arn:${AWS::Partition}:glue:${AWS::Region}:${AWS::AccountId}:database/default"                  


  EMRServiceRole:
    Metadata:
      cfn_nag:
        rules_to_suppress:
          - reason: AWS Managed Policy AmazonEMRServicePolicy_v2 requires a * for some actions
            id: W11  
    DependsOn:
      - CheckResourceExists  
    Type: "AWS::IAM::Role"
    Properties:
      Path: "/"
      Description: "Allows Elastic MapReduce to call AWS services such as EC2 on your behalf."
      AssumeRolePolicyDocument:
        Version: 2008-10-17
        Statement:
          - Effect: Allow
            Principal:
              Service: !Sub elasticmapreduce.${AWS::URLSuffix}
            Action: sts:AssumeRole
            Condition:
              StringEquals:
                aws:SourceAccount: !Sub ${AWS::AccountId}
              ArnLike:
                aws:SourceArn: !Sub arn:${AWS::Partition}:elasticmapreduce:${AWS::Region}:${AWS::AccountId}:* 
      Policies:
      - PolicyDocument:
          Statement:
            - Action:
                - iam:CreateServiceLinkedRole
                - iam:PutRolePolicy
              Effect: Allow
              Resource: !Sub arn:${AWS::Partition}:iam::*:role/aws-service-role/elasticmapreduce.amazonaws.com*/AWSServiceRoleForEMRCleanup*
              Condition:
                StringLike:
                  iam:AWSServiceName:
                  - elasticmapreduce.amazonaws.com
                  - elasticmapreduce.amazonaws.com.cn
          Version: 2012-10-17
        PolicyName: AllowServiceLinkedRole      
      - PolicyDocument:
          Version: "2012-10-17"
          Statement:
          - Condition:
              StringEquals:
                aws:ResourceTag/for-use-with-amazon-emr-managed-policies: "true"
            Resource:
            - !Sub "arn:${AWS::Partition}:ec2:*:*:subnet/*"
            - !Sub "arn:${AWS::Partition}:ec2:*:*:security-group/*"
            Action:
            - "ec2:CreateNetworkInterface"
            - "ec2:RunInstances"
            - "ec2:CreateFleet"
            - "ec2:CreateLaunchTemplate"
            - "ec2:CreateLaunchTemplateVersion"
            Effect: "Allow"
            Sid: "CreateInTaggedNetwork"
          - Condition:
              StringEquals:
                aws:ResourceTag/for-use-with-amazon-emr-managed-policies: "true"
            Resource: !Sub "arn:${AWS::Partition}:ec2:*:*:launch-template/*"
            Action:
            - "ec2:CreateFleet"
            - "ec2:RunInstances"
            - "ec2:CreateLaunchTemplateVersion"
            Effect: "Allow"
            Sid: "CreateWithEMRTaggedLaunchTemplate"
          - Condition:
              StringEquals:
                aws:RequestTag/for-use-with-amazon-emr-managed-policies: "true"
            Resource: !Sub "arn:${AWS::Partition}:ec2:*:*:launch-template/*"
            Action: "ec2:CreateLaunchTemplate"
            Effect: "Allow"
            Sid: "CreateEMRTaggedLaunchTemplate"
          - Condition:
              StringEquals:
                aws:RequestTag/for-use-with-amazon-emr-managed-policies: "true"
            Resource:
            - !Sub "arn:${AWS::Partition}:ec2:*:*:instance/*"
            - !Sub "arn:${AWS::Partition}:ec2:*:*:volume/*"
            Action:
            - "ec2:RunInstances"
            - "ec2:CreateFleet"
            Effect: "Allow"
            Sid: "CreateEMRTaggedInstancesAndVolumes"
          - Resource:
            - !Sub "arn:${AWS::Partition}:ec2:*:*:network-interface/*"
            - !Sub "arn:${AWS::Partition}:ec2:*::image/ami-*"
            - !Sub "arn:${AWS::Partition}:ec2:*:*:key-pair/*"
            - !Sub "arn:${AWS::Partition}:ec2:*:*:capacity-reservation/*"
            - !Sub "arn:${AWS::Partition}:ec2:*:*:placement-group/EMR_*"
            - !Sub "arn:${AWS::Partition}:ec2:*:*:fleet/*"
            - !Sub "arn:${AWS::Partition}:ec2:*:*:dedicated-host/*"
            - !Sub "arn:${AWS::Partition}:resource-groups:*:*:group/*"
            Action:
            - "ec2:RunInstances"
            - "ec2:CreateFleet"
            - "ec2:CreateLaunchTemplate"
            - "ec2:CreateLaunchTemplateVersion"
            Effect: "Allow"
            Sid: "ResourcesToLaunchEC2"
          - Condition:
              StringEquals:
                aws:ResourceTag/for-use-with-amazon-emr-managed-policies: "true"
            Resource: !Sub "arn:${AWS::Partition}:ec2:*:*:*"
            Action:
            - "ec2:CreateLaunchTemplateVersion"
            - "ec2:DeleteLaunchTemplate"
            - "ec2:DeleteNetworkInterface"
            - "ec2:ModifyInstanceAttribute"
            - "ec2:TerminateInstances"
            Effect: "Allow"
            Sid: "ManageEMRTaggedResources"
          - Condition:
              StringEquals:
                aws:ResourceTag/for-use-with-amazon-emr-managed-policies: "true"
            Resource:
            - !Sub "arn:${AWS::Partition}:ec2:*:*:instance/*"
            - !Sub "arn:${AWS::Partition}:ec2:*:*:volume/*"
            - !Sub "arn:${AWS::Partition}:ec2:*:*:network-interface/*"
            - !Sub "arn:${AWS::Partition}:ec2:*:*:launch-template/*"
            Action:
            - "ec2:CreateTags"
            - "ec2:DeleteTags"
            Effect: "Allow"
            Sid: "ManageTagsOnEMRTaggedResources"
          - Condition:
              StringEquals:
                aws:RequestTag/for-use-with-amazon-emr-managed-policies: "true"
            Resource:
            - !Sub "arn:${AWS::Partition}:ec2:*:*:network-interface/*"
            Action:
            - "ec2:CreateNetworkInterface"
            Effect: "Allow"
            Sid: "CreateNetworkInterfaceNeededForPrivateSubnet"
          - Condition:
              StringEquals:
                ec2:CreateAction:
                - "RunInstances"
                - "CreateFleet"
                - "CreateLaunchTemplate"
                - "CreateNetworkInterface"
            Resource:
            - !Sub "arn:${AWS::Partition}:ec2:*:*:network-interface/*"
            - !Sub "arn:${AWS::Partition}:ec2:*:*:instance/*"
            - !Sub "arn:${AWS::Partition}:ec2:*:*:volume/*"
            - !Sub "arn:${AWS::Partition}:ec2:*:*:launch-template/*"
            Action:
            - "ec2:CreateTags"
            Effect: "Allow"
            Sid: "TagOnCreateTaggedEMRResources"
          - Resource:
            - !Sub "arn:${AWS::Partition}:ec2:*:*:placement-group/EMR_*"
            Action:
            - "ec2:CreateTags"
            - "ec2:DeleteTags"
            Effect: "Allow"
            Sid: "TagPlacementGroups"
          - Resource: "*"
            Action:
            - "ec2:DescribeAccountAttributes"
            - "ec2:DescribeCapacityReservations"
            - "ec2:DescribeDhcpOptions"
            - "ec2:DescribeImages"
            - "ec2:DescribeInstances"
            - "ec2:DescribeInstanceTypeOfferings"
            - "ec2:DescribeLaunchTemplates"
            - "ec2:DescribeNetworkAcls"
            - "ec2:DescribeNetworkInterfaces"
            - "ec2:DescribePlacementGroups"
            - "ec2:DescribeRouteTables"
            - "ec2:DescribeSecurityGroups"
            - "ec2:DescribeSubnets"
            - "ec2:DescribeVolumes"
            - "ec2:DescribeVolumeStatus"
            - "ec2:DescribeVpcAttribute"
            - "ec2:DescribeVpcEndpoints"
            - "ec2:DescribeVpcs"
            Effect: "Allow"
            Sid: "ListActionsForEC2Resources"
          - Condition:
              StringEquals:
                aws:RequestTag/for-use-with-amazon-emr-managed-policies: "true"
            Resource:
            - !Sub "arn:${AWS::Partition}:ec2:*:*:security-group/*"
            Action:
            - "ec2:CreateSecurityGroup"
            Effect: "Allow"
            Sid: "CreateDefaultSecurityGroupWithEMRTags"
          - Condition:
              StringEquals:
                aws:ResourceTag/for-use-with-amazon-emr-managed-policies: "true"
            Resource:
            - !Sub "arn:${AWS::Partition}:ec2:*:*:vpc/*"
            Action:
            - "ec2:CreateSecurityGroup"
            Effect: "Allow"
            Sid: "CreateDefaultSecurityGroupInVPCWithEMRTags"
          - Condition:
              StringEquals:
                aws:RequestTag/for-use-with-amazon-emr-managed-policies: "true"
                ec2:CreateAction: "CreateSecurityGroup"
            Resource: !Sub "arn:${AWS::Partition}:ec2:*:*:security-group/*"
            Action:
            - "ec2:CreateTags"
            Effect: "Allow"
            Sid: "TagOnCreateDefaultSecurityGroupWithEMRTags"
          - Condition:
              StringEquals:
                aws:ResourceTag/for-use-with-amazon-emr-managed-policies: "true"
            Resource:
            - !Sub "arn:${AWS::Partition}:ec2:${AWS::Region}:${AWS::AccountId}:security-group/*"
            - !Sub "arn:${AWS::Partition}:ec2:${AWS::Region}:${AWS::AccountId}:security-group-rule/*"
            Action:
            - "ec2:AuthorizeSecurityGroupEgress"
            - "ec2:AuthorizeSecurityGroupIngress"
            - "ec2:RevokeSecurityGroupEgress"
            - "ec2:RevokeSecurityGroupIngress"
            Effect: "Allow"
            Sid: "ManageSecurityGroups"
          - Resource: !Sub "arn:${AWS::Partition}:ec2:*:*:placement-group/EMR_*"
            Action:
            - "ec2:CreatePlacementGroup"
            Effect: "Allow"
            Sid: "CreateEMRPlacementGroups"
          - Resource: !Sub "arn:${AWS::Partition}:ec2:${AWS::Region}:${AWS::AccountId}:placement-group/*"
            Action:
            - "ec2:DeletePlacementGroup"
            Effect: "Allow"
            Sid: "DeletePlacementGroups"
          - Resource: "*"
            Action:
            - "application-autoscaling:DescribeScalableTargets"
            - "application-autoscaling:DescribeScalingPolicies"
            Effect: "Allow"
            Sid: "AutoScaling"
          - Resource: !Sub "arn:${AWS::Partition}:application-autoscaling:${AWS::Region}:${AWS::AccountId}:scalable-target/*"
            Action:
            - "application-autoscaling:DeleteScalingPolicy"
            - "application-autoscaling:DeregisterScalableTarget"
            - "application-autoscaling:PutScalingPolicy"
            - "application-autoscaling:RegisterScalableTarget"
            Effect: "Allow"
            Sid: "AutoScaling2"
          - Resource: !Sub "arn:${AWS::Partition}:resource-groups:${AWS::Region}:${AWS::AccountId}:group/*"
            Action:
            - "resource-groups:ListGroupResources"
            Effect: "Allow"
            Sid: "ResourceGroupsForCapacityReservations"
          - Resource: !Sub "arn:${AWS::Partition}:cloudwatch:*:*:alarm:*_EMR_Auto_Scaling"
            Action:
            - "cloudwatch:PutMetricAlarm"
            - "cloudwatch:DeleteAlarms"
            - "cloudwatch:DescribeAlarms"
            Effect: "Allow"
            Sid: "AutoScalingCloudWatch"
          - Condition:
              StringLike:
                iam:PassedToService: "application-autoscaling.amazonaws.com*"
            Resource: !Sub "arn:${AWS::Partition}:iam::*:role/EMR_AutoScaling_DefaultRole"
            Action: "iam:PassRole"
            Effect: "Allow"
            Sid: "PassRoleForAutoScaling"
          - Condition:
              StringLike:
                iam:PassedToService: "ec2.amazonaws.com*"
            Resource: !Sub "arn:${AWS::Partition}:iam::*:role/EMR_EC2_DefaultRole"
            Action: "iam:PassRole"
            Effect: "Allow"
            Sid: "PassRoleForEC2"
        PolicyName: "EMRServicePolicyfromManagedV2"
      - PolicyDocument:
          Version: "2012-10-17"
          Statement:
          - Resource:
            - !Sub "arn:${AWS::Partition}:ec2:*:*:subnet/*"
            - !Sub "arn:${AWS::Partition}:ec2:*:*:security-group/*"
            - !Sub "arn:${AWS::Partition}:ec2:*:*:security-group/*"
            Action:
            - "ec2:CreateNetworkInterface"
            - "ec2:RunInstances"
            - "ec2:CreateFleet"
            - "ec2:CreateLaunchTemplate"
            - "ec2:CreateLaunchTemplateVersion"
            Effect: "Allow"
            Sid: "CreateInNetwork"
          - Resource:
            - !Sub "arn:${AWS::Partition}:ec2:*:*:security-group/*"
            - !Sub "arn:${AWS::Partition}:ec2:*:*:security-group/*"
            Action:
            - "ec2:AuthorizeSecurityGroupEgress"
            - "ec2:AuthorizeSecurityGroupIngress"
            - "ec2:RevokeSecurityGroupEgress"
            - "ec2:RevokeSecurityGroupIngress"
            Effect: "Allow"
            Sid: "ManageSecurityGroups"
          - Resource:
            - !Sub "arn:${AWS::Partition}:ec2:*:*:vpc/*"
            Action:
            - "ec2:CreateSecurityGroup"
            Effect: "Allow"
            Sid: "CreateDefaultSecurityGroupInVPC"
          - Condition:
              StringLike:
                iam:PassedToService: "ec2.amazonaws.com"
            Resource: !GetAtt EMREc2Role.Arn
            Action: "iam:PassRole"
            Effect: "Allow"
            Sid: "PassRoleForEC2"
        PolicyName: "EMRServiceRolePolicy1"


     



############################ End Code ########################################


############################################################ End Main Body ###################################################################



Outputs:
  BucketName:
    Value: !Ref EMRLogS3Bucket
    Description: Solution Amazon S3 Bucket to store the EMR Spark Submit PySpark Script and the EMR Logs

  EMREC2StateMachineArn:
    Value: !Ref EMREC2StateMachine
    Description: StateMachine ARN. Please goto AWS Step Function Management Console, choose this State Machine and Start Execution to commence Migration

  EMREC2RoleArn:
    Value: !GetAtt EMREc2Role.Arn
    Description: EMR Cluster EC2 Role. Please remember to grant this role access to KMS encryptions keys on your source bucket!

  PySparkScriptURI:
    Value: !Join ['', ['s3://', !Ref EMRLogS3Bucket, '/', !FindInMap [ PySpark, Script, s3key ] ]]  
    Description: Location of the EMR Spark-Submit PySpark script. 

    
